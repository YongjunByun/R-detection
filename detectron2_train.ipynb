{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trainer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1P3plzfiuGZz43Up0mqzQ92aWVhauDVyd",
      "authorship_tag": "ABX9TyPhVZBB+PcIgiRgCjs3hYSg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "PjWhfsmK5LUj",
        "outputId": "6f75754c-9a41-4139-96db-92cf8c6f4ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.10.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.10.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (2137.7 MB)\n",
            "\u001b[K     |████████████▌                   | 834.1 MB 1.6 MB/s eta 0:13:41tcmalloc: large alloc 1147494400 bytes == 0x3995c000 @  0x7f731b372615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |███████████████▉                | 1055.7 MB 1.5 MB/s eta 0:11:47tcmalloc: large alloc 1434370048 bytes == 0x7dfb2000 @  0x7f731b372615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████            | 1336.2 MB 1.5 MB/s eta 0:08:41tcmalloc: large alloc 1792966656 bytes == 0x2de4000 @  0x7f731b372615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████████████▎      | 1691.1 MB 1.4 MB/s eta 0:05:18tcmalloc: large alloc 2241208320 bytes == 0x6dbcc000 @  0x7f731b372615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 2137.7 MB 1.4 MB/s eta 0:00:01tcmalloc: large alloc 2137653248 bytes == 0xf352e000 @  0x7f731b3711e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n",
            "tcmalloc: large alloc 2672066560 bytes == 0x1e7052000 @  0x7f731b372615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n",
            "\u001b[K     |████████████████████████████████| 2137.7 MB 406 bytes/s \n",
            "\u001b[?25hCollecting torchvision==0.11.2+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.11.2%2Bcu111-cp37-cp37m-linux_x86_64.whl (24.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.5 MB 2.3 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.10.1\n",
            "  Downloading https://download.pytorch.org/whl/rocm4.1/torchaudio-0.10.1%2Brocm4.1-cp37-cp37m-linux_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 35.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.1+cu111) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.2+cu111) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.2+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.11.0+cu113\n",
            "    Uninstalling torchaudio-0.11.0+cu113:\n",
            "      Successfully uninstalled torchaudio-0.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.1+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.1+cu111 torchaudio-0.10.1+rocm4.1 torchvision-0.11.2+cu111\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyMo_xEmUWgu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25c2e80b-1b39-4ecd-9516-972ce770d591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml==5.1 in /usr/local/lib/python3.7/dist-packages (5.1)\n",
            "torch:  1.10 ; cuda:  cu111\n",
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html\n",
            "Collecting detectron2\n",
            "  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/detectron2-0.6%2Bcu111-cp37-cp37m-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 963 kB/s \n",
            "\u001b[?25hCollecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.1.2-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.64.0)\n",
            "Collecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.1.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 37.6 MB/s \n",
            "\u001b[?25hCollecting black==21.4b2\n",
            "  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 47.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.8.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20220506.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (7.1.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting pathspec<1,>=0.8.1\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting toml>=0.10.1\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (4.2.0)\n",
            "Collecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 32.6 MB/s \n",
            "\u001b[?25hCollecting regex>=2020.1.8\n",
            "  Downloading regex-2022.4.24-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (5.1)\n",
            "Collecting importlib-resources<5.3\n",
            "  Downloading importlib_resources-5.2.3-py3-none-any.whl (27 kB)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 20.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources<5.3->hydra-core>=1.1->detectron2) (3.8.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (2.23.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.44.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (4.11.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.2.0)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20220506-py3-none-any.whl size=61284 sha256=2b22e3bb99d1893a44bbf1eebd6bc71d357d00270b59376a19daf13578a341d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/ef/3c/708de8799f89f0871bd209866831fe3885db93fa090608fa73\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=60eb46df038bcc637ea3ee5228c7ae550e7479c8e0652435fc7d4133107d8707\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: portalocker, antlr4-python3-runtime, yacs, typed-ast, toml, regex, pathspec, omegaconf, mypy-extensions, iopath, importlib-resources, hydra-core, fvcore, black, detectron2\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 5.7.1\n",
            "    Uninstalling importlib-resources-5.7.1:\n",
            "      Successfully uninstalled importlib-resources-5.7.1\n",
            "Successfully installed antlr4-python3-runtime-4.8 black-21.4b2 detectron2-0.6+cu111 fvcore-0.1.5.post20220506 hydra-core-1.1.2 importlib-resources-5.2.3 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.1.2 pathspec-0.9.0 portalocker-2.4.0 regex-2022.4.24 toml-0.10.2 typed-ast-1.5.3 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install pyyaml==5.1\n",
        "\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "# Install detectron2 that matches the above pytorch version\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n",
        "\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os, json\n",
        "import pandas as pd\n",
        "data_train = json.load(open(\"/content/drive/MyDrive/detectron/dataset/rhun_train/train.json\"))\n",
        "data_val = json.load(open(\"/content/drive/MyDrive/detectron/dataset/rhun_val/val.json\"))\n",
        "df_train = pd.DataFrame(data_train[\"data\"])\n",
        "df_val = pd.DataFrame(data_val[\"data\"])"
      ],
      "metadata": {
        "id": "nBamVWHZmsnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIbAM2pv-urF"
      },
      "source": [
        "from detectron2.structures import BoxMode\n",
        "#/content/drive/MyDrive/detectron/dataset/training_images\n",
        "\n",
        "def get_data_dicts(img_directory, df_train):\n",
        "    img_dir = img_directory\n",
        "    dataset_dicts = []\n",
        "    for i in range(len(df_train)):\n",
        "        record = {}\n",
        "\n",
        "        index = df_train[\"index\"][i]\n",
        "        filename = os.path.join(img_dir, df_train[\"image\"][i])\n",
        "        height, width = cv2.imread(filename).shape[:2]\n",
        "\n",
        "        record[\"file_name\"] = filename\n",
        "        record[\"image_id\"] = index\n",
        "        record[\"height\"] = height\n",
        "        record[\"width\"] = width\n",
        "\n",
        "        annos = df_train[\"annotations\"]\n",
        "        objs = []\n",
        "        for j in range(len(annos[i])):\n",
        "            x = annos[i][j][\"coordinates\"][\"x\"]-(0.5*annos[i][j][\"coordinates\"][\"width\"])\n",
        "            y = annos[i][j][\"coordinates\"][\"y\"]-(0.5*annos[i][j][\"coordinates\"][\"height\"])\n",
        "            w = annos[i][j][\"coordinates\"][\"width\"]\n",
        "            h = annos[i][j][\"coordinates\"][\"height\"]\n",
        "            class_num =0 \n",
        "            if annos[i][j][\"label\"] == \"left\":\n",
        "                class_num = 0\n",
        "            elif annos[i][j][\"label\"] == \"right\":\n",
        "                class_num = 1\n",
        "            elif annos[i][j][\"label\"] == \"up\":\n",
        "                class_num = 2\n",
        "            elif annos[i][j][\"label\"] == \"down\":\n",
        "                class_num = 3\n",
        "            obj = {\n",
        "                        \"bbox\": [x, y, w, h],\n",
        "                        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
        "                        \"category_id\": class_num#annos[i][j][\"label\"],\n",
        "                  }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "\n",
        "\n",
        "# DatasetCatalog.register(\"rhun_train\", get_data_dicts(\"/content/drive/MyDrive/detectron/dataset/rhun_train\"))\n",
        "# MetadataCatalog.get(\"rhun_train\").set(thing_classes=[\"left\", \"right\", \"up\", \"down\"])\n",
        "# rhun_metadata = MetadataCatalog.get(\"rhun_train\")\n",
        "for d in [\"train\", \"val\"]:\n",
        "    DatasetCatalog.register(\"rhun_\" + d, lambda d=d: get_data_dicts(\"/content/drive/MyDrive/detectron/dataset/rhun_\" + d, df_train))\n",
        "    MetadataCatalog.get(\"rhun_\" + d).set(thing_classes=[\"left\", \"right\", \"up\", \"down\"])\n",
        "\n",
        "\n",
        "rhun_metadata = MetadataCatalog.get(\"rhun_train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear datasets\n",
        "DatasetCatalog.clear()\n",
        "MetadataCatalog.clear()"
      ],
      "metadata": {
        "id": "BgEAf8RvLtg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "dataset_dicts = get_data_dicts(\"/content/drive/MyDrive/detectron/dataset/rhun_train\", df_train)\n",
        "dataset_dicts_val = get_data_dicts(\"/content/drive/MyDrive/detectron/dataset/rhun_val\", df_val)\n"
      ],
      "metadata": {
        "id": "3K6q__-CutTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in random.sample(dataset_dicts, 5):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=rhun_metadata, scale=1.0)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "nt6ktsQveQWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"rhun_train\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.INPUT.RANDOM_FLIP = \"none\"\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg.SOLVER.BASE_LR = 0.00030  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 1000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Q8XUuw3S8kew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d52a272-fa3e-4a03-f8b9-b6bb9f671c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[05/06 09:15:16 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[05/06 09:15:18 d2.data.build]: \u001b[0mRemoved 1 images with no usable annotations. 300 images left.\n",
            "\u001b[32m[05/06 09:15:18 d2.data.build]: \u001b[0mDistribution of instances among all 4 categories:\n",
            "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|    left    | 283          |   right    | 308          |     up     | 306          |\n",
            "|    down    | 275          |            |              |            |              |\n",
            "|   total    | 1172         |            |              |            |              |\u001b[0m\n",
            "\u001b[32m[05/06 09:15:18 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[05/06 09:15:18 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[05/06 09:15:18 d2.data.common]: \u001b[0mSerializing 300 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[05/06 09:15:18 d2.data.common]: \u001b[0mSerialized dataset takes 0.15 MiB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_b275ba.pkl: 167MB [00:04, 38.7MB/s]                           \n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (5, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (16, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (16,) in the model! You might want to double check if this is expected.\n",
            "Some model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[05/06 09:15:32 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  max_size = (max_size + (stride - 1)) // stride * stride\n",
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[05/06 09:16:05 d2.utils.events]: \u001b[0m eta: 0:25:57  iter: 19  total_loss: 2.366  loss_cls: 1.489  loss_box_reg: 0.8383  loss_rpn_cls: 0.02806  loss_rpn_loc: 0.01416  time: 1.5921  data_time: 0.0405  lr: 5.9943e-06  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:16:37 d2.utils.events]: \u001b[0m eta: 0:25:34  iter: 39  total_loss: 2.267  loss_cls: 1.412  loss_box_reg: 0.7874  loss_rpn_cls: 0.02876  loss_rpn_loc: 0.01392  time: 1.5986  data_time: 0.0275  lr: 1.1988e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:17:09 d2.utils.events]: \u001b[0m eta: 0:25:09  iter: 59  total_loss: 2.084  loss_cls: 1.259  loss_box_reg: 0.8013  loss_rpn_cls: 0.02703  loss_rpn_loc: 0.0131  time: 1.6076  data_time: 0.0279  lr: 1.7982e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:17:42 d2.utils.events]: \u001b[0m eta: 0:24:46  iter: 79  total_loss: 1.912  loss_cls: 1.055  loss_box_reg: 0.8113  loss_rpn_cls: 0.02029  loss_rpn_loc: 0.01276  time: 1.6127  data_time: 0.0291  lr: 2.3976e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:18:15 d2.utils.events]: \u001b[0m eta: 0:24:17  iter: 99  total_loss: 1.716  loss_cls: 0.8658  loss_box_reg: 0.8235  loss_rpn_cls: 0.01839  loss_rpn_loc: 0.0117  time: 1.6155  data_time: 0.0307  lr: 2.997e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:18:47 d2.utils.events]: \u001b[0m eta: 0:23:44  iter: 119  total_loss: 1.661  loss_cls: 0.7606  loss_box_reg: 0.8513  loss_rpn_cls: 0.01361  loss_rpn_loc: 0.01231  time: 1.6172  data_time: 0.0294  lr: 3.5964e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:19:19 d2.utils.events]: \u001b[0m eta: 0:23:12  iter: 139  total_loss: 1.626  loss_cls: 0.712  loss_box_reg: 0.8743  loss_rpn_cls: 0.01212  loss_rpn_loc: 0.0132  time: 1.6169  data_time: 0.0268  lr: 4.1958e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:19:52 d2.utils.events]: \u001b[0m eta: 0:22:39  iter: 159  total_loss: 1.576  loss_cls: 0.6574  loss_box_reg: 0.8968  loss_rpn_cls: 0.009711  loss_rpn_loc: 0.01268  time: 1.6164  data_time: 0.0259  lr: 4.7952e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:20:24 d2.utils.events]: \u001b[0m eta: 0:22:05  iter: 179  total_loss: 1.544  loss_cls: 0.6301  loss_box_reg: 0.8869  loss_rpn_cls: 0.007732  loss_rpn_loc: 0.01138  time: 1.6160  data_time: 0.0254  lr: 5.3946e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:20:56 d2.utils.events]: \u001b[0m eta: 0:21:33  iter: 199  total_loss: 1.51  loss_cls: 0.5903  loss_box_reg: 0.9021  loss_rpn_cls: 0.006247  loss_rpn_loc: 0.01082  time: 1.6159  data_time: 0.0286  lr: 5.994e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:21:28 d2.utils.events]: \u001b[0m eta: 0:21:00  iter: 219  total_loss: 1.462  loss_cls: 0.5681  loss_box_reg: 0.8787  loss_rpn_cls: 0.005783  loss_rpn_loc: 0.01133  time: 1.6152  data_time: 0.0236  lr: 6.5934e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:22:01 d2.utils.events]: \u001b[0m eta: 0:20:27  iter: 239  total_loss: 1.418  loss_cls: 0.5362  loss_box_reg: 0.8648  loss_rpn_cls: 0.004797  loss_rpn_loc: 0.01031  time: 1.6150  data_time: 0.0243  lr: 7.1928e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:22:33 d2.utils.events]: \u001b[0m eta: 0:19:55  iter: 259  total_loss: 1.375  loss_cls: 0.5133  loss_box_reg: 0.8452  loss_rpn_cls: 0.003717  loss_rpn_loc: 0.01031  time: 1.6151  data_time: 0.0285  lr: 7.7922e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:23:05 d2.utils.events]: \u001b[0m eta: 0:19:23  iter: 279  total_loss: 1.32  loss_cls: 0.4908  loss_box_reg: 0.819  loss_rpn_cls: 0.003098  loss_rpn_loc: 0.01079  time: 1.6153  data_time: 0.0283  lr: 8.3916e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:23:38 d2.utils.events]: \u001b[0m eta: 0:18:51  iter: 299  total_loss: 1.263  loss_cls: 0.4664  loss_box_reg: 0.7808  loss_rpn_cls: 0.004107  loss_rpn_loc: 0.01028  time: 1.6157  data_time: 0.0273  lr: 8.991e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:24:10 d2.utils.events]: \u001b[0m eta: 0:18:19  iter: 319  total_loss: 1.189  loss_cls: 0.4403  loss_box_reg: 0.7319  loss_rpn_cls: 0.002109  loss_rpn_loc: 0.01141  time: 1.6163  data_time: 0.0274  lr: 9.5904e-05  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:24:43 d2.utils.events]: \u001b[0m eta: 0:17:47  iter: 339  total_loss: 1.112  loss_cls: 0.417  loss_box_reg: 0.6808  loss_rpn_cls: 0.001615  loss_rpn_loc: 0.01143  time: 1.6166  data_time: 0.0259  lr: 0.0001019  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:25:15 d2.utils.events]: \u001b[0m eta: 0:17:15  iter: 359  total_loss: 0.989  loss_cls: 0.3981  loss_box_reg: 0.5751  loss_rpn_cls: 0.001599  loss_rpn_loc: 0.01125  time: 1.6174  data_time: 0.0272  lr: 0.00010789  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:25:48 d2.utils.events]: \u001b[0m eta: 0:16:43  iter: 379  total_loss: 0.8177  loss_cls: 0.3691  loss_box_reg: 0.4397  loss_rpn_cls: 0.001744  loss_rpn_loc: 0.01227  time: 1.6183  data_time: 0.0263  lr: 0.00011389  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:26:21 d2.utils.events]: \u001b[0m eta: 0:16:11  iter: 399  total_loss: 0.66  loss_cls: 0.3395  loss_box_reg: 0.3068  loss_rpn_cls: 0.001229  loss_rpn_loc: 0.01294  time: 1.6193  data_time: 0.0281  lr: 0.00011988  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:26:54 d2.utils.events]: \u001b[0m eta: 0:15:39  iter: 419  total_loss: 0.5729  loss_cls: 0.3085  loss_box_reg: 0.2545  loss_rpn_cls: 0.0007135  loss_rpn_loc: 0.01208  time: 1.6205  data_time: 0.0315  lr: 0.00012587  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:27:27 d2.utils.events]: \u001b[0m eta: 0:15:07  iter: 439  total_loss: 0.5104  loss_cls: 0.2697  loss_box_reg: 0.2225  loss_rpn_cls: 0.0006379  loss_rpn_loc: 0.01104  time: 1.6213  data_time: 0.0271  lr: 0.00013187  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:27:59 d2.utils.events]: \u001b[0m eta: 0:14:35  iter: 459  total_loss: 0.4805  loss_cls: 0.251  loss_box_reg: 0.2188  loss_rpn_cls: 0.000491  loss_rpn_loc: 0.01131  time: 1.6220  data_time: 0.0270  lr: 0.00013786  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:28:32 d2.utils.events]: \u001b[0m eta: 0:14:03  iter: 479  total_loss: 0.4365  loss_cls: 0.2065  loss_box_reg: 0.2092  loss_rpn_cls: 0.0005589  loss_rpn_loc: 0.01194  time: 1.6225  data_time: 0.0264  lr: 0.00014386  max_mem: 4337M\n",
            "\u001b[32m[05/06 09:29:05 d2.utils.events]: \u001b[0m eta: 0:13:31  iter: 499  total_loss: 0.3916  loss_cls: 0.1761  loss_box_reg: 0.2022  loss_rpn_cls: 0.0003361  loss_rpn_loc: 0.01025  time: 1.6228  data_time: 0.0243  lr: 0.00014985  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:29:37 d2.utils.events]: \u001b[0m eta: 0:12:59  iter: 519  total_loss: 0.3672  loss_cls: 0.1672  loss_box_reg: 0.1939  loss_rpn_cls: 0.0007967  loss_rpn_loc: 0.01092  time: 1.6231  data_time: 0.0268  lr: 0.00015584  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:30:10 d2.utils.events]: \u001b[0m eta: 0:12:26  iter: 539  total_loss: 0.3336  loss_cls: 0.1377  loss_box_reg: 0.1888  loss_rpn_cls: 0.000686  loss_rpn_loc: 0.01152  time: 1.6232  data_time: 0.0239  lr: 0.00016184  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:30:42 d2.utils.events]: \u001b[0m eta: 0:11:54  iter: 559  total_loss: 0.3266  loss_cls: 0.1312  loss_box_reg: 0.1897  loss_rpn_cls: 0.0002395  loss_rpn_loc: 0.01077  time: 1.6233  data_time: 0.0231  lr: 0.00016783  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:31:15 d2.utils.events]: \u001b[0m eta: 0:11:21  iter: 579  total_loss: 0.3062  loss_cls: 0.1122  loss_box_reg: 0.1832  loss_rpn_cls: 0.0002006  loss_rpn_loc: 0.01087  time: 1.6234  data_time: 0.0269  lr: 0.00017383  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:31:48 d2.utils.events]: \u001b[0m eta: 0:10:49  iter: 599  total_loss: 0.3014  loss_cls: 0.1001  loss_box_reg: 0.1791  loss_rpn_cls: 0.0005533  loss_rpn_loc: 0.01056  time: 1.6235  data_time: 0.0244  lr: 0.00017982  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:32:20 d2.utils.events]: \u001b[0m eta: 0:10:17  iter: 619  total_loss: 0.2848  loss_cls: 0.0897  loss_box_reg: 0.1789  loss_rpn_cls: 0.0002867  loss_rpn_loc: 0.01022  time: 1.6236  data_time: 0.0276  lr: 0.00018581  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:32:53 d2.utils.events]: \u001b[0m eta: 0:09:44  iter: 639  total_loss: 0.2736  loss_cls: 0.08245  loss_box_reg: 0.1803  loss_rpn_cls: 0.0003651  loss_rpn_loc: 0.009941  time: 1.6238  data_time: 0.0283  lr: 0.00019181  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:33:25 d2.utils.events]: \u001b[0m eta: 0:09:12  iter: 659  total_loss: 0.2709  loss_cls: 0.07693  loss_box_reg: 0.1802  loss_rpn_cls: 0.0002929  loss_rpn_loc: 0.01071  time: 1.6238  data_time: 0.0274  lr: 0.0001978  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:33:58 d2.utils.events]: \u001b[0m eta: 0:08:39  iter: 679  total_loss: 0.257  loss_cls: 0.07123  loss_box_reg: 0.1806  loss_rpn_cls: 0.0004273  loss_rpn_loc: 0.01001  time: 1.6238  data_time: 0.0250  lr: 0.0002038  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:34:30 d2.utils.events]: \u001b[0m eta: 0:08:07  iter: 699  total_loss: 0.2598  loss_cls: 0.06881  loss_box_reg: 0.1724  loss_rpn_cls: 0.0004769  loss_rpn_loc: 0.009751  time: 1.6239  data_time: 0.0262  lr: 0.00020979  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:35:03 d2.utils.events]: \u001b[0m eta: 0:07:34  iter: 719  total_loss: 0.2541  loss_cls: 0.06407  loss_box_reg: 0.18  loss_rpn_cls: 8.373e-05  loss_rpn_loc: 0.009781  time: 1.6239  data_time: 0.0266  lr: 0.00021578  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:35:35 d2.utils.events]: \u001b[0m eta: 0:07:02  iter: 739  total_loss: 0.2438  loss_cls: 0.06548  loss_box_reg: 0.1667  loss_rpn_cls: 0.0002119  loss_rpn_loc: 0.009461  time: 1.6239  data_time: 0.0266  lr: 0.00022178  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:36:08 d2.utils.events]: \u001b[0m eta: 0:06:29  iter: 759  total_loss: 0.2399  loss_cls: 0.06004  loss_box_reg: 0.1722  loss_rpn_cls: 8.117e-05  loss_rpn_loc: 0.009581  time: 1.6238  data_time: 0.0261  lr: 0.00022777  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:36:40 d2.utils.events]: \u001b[0m eta: 0:05:57  iter: 779  total_loss: 0.2324  loss_cls: 0.05742  loss_box_reg: 0.1699  loss_rpn_cls: 0.0001833  loss_rpn_loc: 0.00942  time: 1.6237  data_time: 0.0269  lr: 0.00023377  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:37:13 d2.utils.events]: \u001b[0m eta: 0:05:24  iter: 799  total_loss: 0.2394  loss_cls: 0.05506  loss_box_reg: 0.1749  loss_rpn_cls: 0.0004137  loss_rpn_loc: 0.009361  time: 1.6237  data_time: 0.0273  lr: 0.00023976  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:37:45 d2.utils.events]: \u001b[0m eta: 0:04:52  iter: 819  total_loss: 0.2343  loss_cls: 0.05671  loss_box_reg: 0.1699  loss_rpn_cls: 0.0001562  loss_rpn_loc: 0.009003  time: 1.6236  data_time: 0.0275  lr: 0.00024575  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:38:17 d2.utils.events]: \u001b[0m eta: 0:04:19  iter: 839  total_loss: 0.2374  loss_cls: 0.05363  loss_box_reg: 0.1677  loss_rpn_cls: 8.814e-05  loss_rpn_loc: 0.008692  time: 1.6235  data_time: 0.0263  lr: 0.00025175  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:38:51 d2.utils.events]: \u001b[0m eta: 0:03:47  iter: 859  total_loss: 0.2253  loss_cls: 0.05581  loss_box_reg: 0.1646  loss_rpn_cls: 0.0001306  loss_rpn_loc: 0.009195  time: 1.6244  data_time: 0.0285  lr: 0.00025774  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:39:24 d2.utils.events]: \u001b[0m eta: 0:03:14  iter: 879  total_loss: 0.2174  loss_cls: 0.05088  loss_box_reg: 0.1588  loss_rpn_cls: 0.0001108  loss_rpn_loc: 0.009377  time: 1.6256  data_time: 0.0342  lr: 0.00026374  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:39:57 d2.utils.events]: \u001b[0m eta: 0:02:42  iter: 899  total_loss: 0.2212  loss_cls: 0.05109  loss_box_reg: 0.1643  loss_rpn_cls: 7.953e-05  loss_rpn_loc: 0.008662  time: 1.6259  data_time: 0.0305  lr: 0.00026973  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:40:30 d2.utils.events]: \u001b[0m eta: 0:02:09  iter: 919  total_loss: 0.2154  loss_cls: 0.04948  loss_box_reg: 0.1552  loss_rpn_cls: 0.0001205  loss_rpn_loc: 0.008258  time: 1.6261  data_time: 0.0306  lr: 0.00027572  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:41:03 d2.utils.events]: \u001b[0m eta: 0:01:37  iter: 939  total_loss: 0.2072  loss_cls: 0.04646  loss_box_reg: 0.1549  loss_rpn_cls: 5.563e-05  loss_rpn_loc: 0.007944  time: 1.6264  data_time: 0.0303  lr: 0.00028172  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:41:35 d2.utils.events]: \u001b[0m eta: 0:01:04  iter: 959  total_loss: 0.2257  loss_cls: 0.05132  loss_box_reg: 0.1597  loss_rpn_cls: 9.225e-05  loss_rpn_loc: 0.009105  time: 1.6268  data_time: 0.0299  lr: 0.00028771  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:42:08 d2.utils.events]: \u001b[0m eta: 0:00:32  iter: 979  total_loss: 0.2207  loss_cls: 0.04829  loss_box_reg: 0.1598  loss_rpn_cls: 8.602e-05  loss_rpn_loc: 0.009093  time: 1.6271  data_time: 0.0279  lr: 0.00029371  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:42:42 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 999  total_loss: 0.2216  loss_cls: 0.04546  loss_box_reg: 0.1607  loss_rpn_cls: 5.526e-05  loss_rpn_loc: 0.008861  time: 1.6276  data_time: 0.0291  lr: 0.0002997  max_mem: 4338M\n",
            "\u001b[32m[05/06 09:42:43 d2.engine.hooks]: \u001b[0mOverall training speed: 998 iterations in 0:27:04 (1.6276 s / it)\n",
            "\u001b[32m[05/06 09:42:43 d2.engine.hooks]: \u001b[0mTotal training time: 0:27:06 (0:00:02 on hooks)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "metadata": {
        "id": "B-SJPMOdQCRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictor with trained model\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
        "# cfg.DATASETS.TEST = (\"car_testing_images\", )\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "metadata": {
        "id": "gMk151n_9wlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download config settings\n",
        "with open(\"/content/output.yaml\", \"w\") as f: \n",
        "  f.write(cfg.dump()) # save config to file\n"
      ],
      "metadata": {
        "id": "0xmzryCsvBdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing inference time\n",
        "import time\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "im = cv2.imread(\"/content/drive/MyDrive/detectron/dataset/rhun_val/0030.png\")\n",
        "start = time.time()\n",
        "outputs = predictor(im)\n",
        "\n",
        "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시\n",
        "\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.0)\n",
        "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(v.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "4l_A94bW9w6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing valdation sets\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "# dataset_dicts_val = get_data_dicts(\"/content/drive/MyDrive/detectron/dataset/rhun_val\", df_val)\n",
        "for d in random.sample(dataset_dicts_val, 10):    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    start = time.time()\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=rhun_metadata, \n",
        "                   scale=1.0, \n",
        "                   instance_mode=ColorMode.IMAGE   \n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "Gf0TeDfMO1yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# empty cache\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Ao7442rh_q4x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "ef1f59af-292a-423b-d2db-dc364df398d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/19 12:14:23 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
            "\u001b[32m[04/19 12:14:23 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'rhun_val' to COCO format ...\n",
            "\u001b[32m[04/19 12:14:23 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'rhun_val' to COCO format ...)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-caa674660539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCOEvaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_on_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_detection_test_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCOEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rhun_val\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_detection_test_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rhun_val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/evaluation/coco_evaluation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_name, tasks, distributed, output_dir, max_dets_per_image, use_fast_impl, kpt_oks_sigmas)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mcache_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{dataset_name}_coco_format.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mconvert_to_coco_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPathManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/data/datasets/coco.py\u001b[0m in \u001b[0;36mconvert_to_coco_json\u001b[0;34m(dataset_name, output_file, allow_cached)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Converting annotations of dataset '{dataset_name}' to COCO format ...)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mcoco_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_coco_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Caching COCO format annotations at '{output_file}' ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/data/datasets/coco.py\u001b[0m in \u001b[0;36mconvert_to_coco_dict\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \"\"\"\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0mdataset_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetadataCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/data/catalog.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 )\n\u001b[1;32m     57\u001b[0m             ) from e\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-cd0cf05b4214>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# rhun_metadata = MetadataCatalog.get(\"rhun_train\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mDatasetCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rhun_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_data_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/detectron/dataset/rhun_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mMetadataCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rhun_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"up\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"down\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-cd0cf05b4214>\u001b[0m in \u001b[0;36mget_data_dicts\u001b[0;34m(img_directory, df_train)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"/content/output.yaml\")\n",
        "cfg.MODEL.WEIGHTS = (\"/content/drive/MyDrive/detectron/model_final.pth\")\n",
        "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4   # set the testing threshold for this model\n",
        "# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
        "\n",
        "# cfg.DATASETS.TEST = (\"car_testing_images\", )\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "metadata": {
        "id": "NUWmv3jE8Pd3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}